# RTX 3090 å¿«é€Ÿå¼€å§‹æŒ‡å—

## 1. æ£€æŸ¥ç¯å¢ƒ âœ“

ä½ çš„GPUå·²è¯†åˆ«:
- GPU: NVIDIA GeForce RTX 3090
- VRAM: 25.3 GB
- CUDA: 12.1
- PyTorch: 2.4.1+cu121

## 2. å·²ä¼˜åŒ–é…ç½®

### æ€§èƒ½ä¼˜åŒ–
- âœ… Batch Size: 512 (å……åˆ†åˆ©ç”¨æ˜¾å­˜)
- âœ… æ¨¡å‹åŠ å¤§: LSTM 256 hidden (æ›´å¼ºå­¦ä¹ èƒ½åŠ›)
- âœ… æ··åˆç²¾åº¦: FP16è®­ç»ƒ (é€Ÿåº¦æå‡50%)
- âœ… DataLoader: 8 workers (å‡å°‘I/Oç“¶é¢ˆ)
- âœ… CuDNN benchmark (è‡ªåŠ¨ä¼˜åŒ–)

### é¢„æœŸæ€§èƒ½
- è®­ç»ƒé€Ÿåº¦: **0.5ç§’/epoch**
- æ€»æ—¶é—´: **< 1åˆ†é’Ÿ** (100 epochs)
- GPUåˆ©ç”¨ç‡: 85-100%
- æ˜¾å­˜å ç”¨: 4-5GB

## 3. å¼€å§‹è®­ç»ƒ

```bash
# æ–¹æ³•1: ç›´æ¥è¿è¡Œ
cd src
python train.py

# æ–¹æ³•2: åå°è¿è¡Œå¹¶è®°å½•æ—¥å¿—
cd src
nohup python train.py > ../training.log 2>&1 &

# æ–¹æ³•3: ä½¿ç”¨å±å¹•ä¼šè¯
screen -S bc_training
cd src
python train.py
# Ctrl+A, D åˆ†ç¦»ä¼šè¯
# screen -r bc_training æ¢å¤ä¼šè¯
```

## 4. ç›‘æ§è®­ç»ƒ

### ç»ˆç«¯1: è¿è¡Œè®­ç»ƒ
```bash
cd src
python train.py
```

ä½ ä¼šçœ‹åˆ°:
```
Using device: cuda
Using mixed precision training (AMP) for faster training on RTX 3090
Model created with 620,480 parameters

Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1754/1754 [00:00<00:00, 2500it/s]
Train Loss: 0.0234
Val Loss: 0.0189, MAE: 123456 bps, RÂ²: 0.82
```

### ç»ˆç«¯2: ç›‘æ§GPU
```bash
# å®æ—¶ç›‘æ§
watch -n 0.5 nvidia-smi

# ä½ åº”è¯¥çœ‹åˆ°:
# GPU-Util: 95-100%
# Memory: 4500MB / 25088MB
# Power: 300-350W
# Temp: 60-75Â°C
```

### ç»ˆç«¯3: Tensorboard
```bash
tensorboard --logdir logs
# è®¿é—® http://localhost:6006
```

## 5. è®­ç»ƒå®Œæˆå

### æ£€æŸ¥ç»“æœ
```bash
# æŸ¥çœ‹æœ€ä½³æ¨¡å‹
ls -lh checkpoints/best.pt

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -50 training.log

# æŸ¥çœ‹tensorboard
tensorboard --logdir logs
```

### æµ‹è¯•æ¨¡å‹
```python
import torch
from model import GCCBC_LSTM
from config import Config

# åŠ è½½æœ€ä½³æ¨¡å‹
config = Config()
model = GCCBC_LSTM(config)
checkpoint = torch.load('checkpoints/best.pt')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

print(f"Best validation loss: {checkpoint['best_val_loss']:.4f}")
print(f"Trained for {checkpoint['epoch']} epochs")
```

## 6. æ€§èƒ½å¯¹æ¯”

| é…ç½® | é€Ÿåº¦ | æ—¶é—´ | å‚æ•° |
|------|------|------|------|
| åŸå§‹CPU | 30s/epoch | ~50åˆ†é’Ÿ | 155K |
| åŸå§‹GPU | 2.5s/epoch | ~4åˆ†é’Ÿ | 155K |
| **ä¼˜åŒ–RTX3090** | **0.5s/epoch** | **<1åˆ†é’Ÿ** | **620K** |

é€Ÿåº¦æå‡: **50å€+**

## 7. æ•…éšœæ’æŸ¥

### æ˜¾å­˜ä¸è¶³
å¦‚æœå‡ºç° "CUDA out of memory":
```python
# src/config.py
BATCH_SIZE = 256  # å‡å°åˆ°256
# æˆ–
BATCH_SIZE = 128  # è¿›ä¸€æ­¥å‡å°
```

### è®­ç»ƒå¾ˆæ…¢
æ£€æŸ¥æ˜¯å¦åœ¨ä½¿ç”¨GPU:
```bash
python -c "import torch; print(torch.cuda.is_available())"
# åº”è¯¥è¾“å‡º: True
```

### AMPé”™è¯¯
å¦‚æœæ··åˆç²¾åº¦æœ‰é—®é¢˜:
```python
# src/config.py
USE_AMP = False  # å…³é—­AMP
```

## 8. æ¨èå·¥ä½œæµ

```bash
# ç¬¬ä¸€æ¬¡è®­ç»ƒï¼ˆå¿«é€ŸéªŒè¯ï¼‰
cd src
python train.py  # åº”è¯¥<1åˆ†é’Ÿå®Œæˆ

# å¦‚æœæ•ˆæœå¥½ï¼Œç»§ç»­è®­ç»ƒæ›´å¤šepochs
# ç¼–è¾‘ config.py: NUM_EPOCHS = 200
python train.py

# æŸ¥çœ‹ç»“æœ
tensorboard --logdir ../logs
```

## 9. ä¸‹ä¸€æ­¥

è®­ç»ƒå®Œæˆå:
1. æŸ¥çœ‹ `reports/` ç›®å½•çš„è®­ç»ƒæ›²çº¿
2. åˆ†ææ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°
3. è€ƒè™‘ç”¨äºå¼ºåŒ–å­¦ä¹ fine-tuning
4. éƒ¨ç½²åˆ°å®é™…WebRTCç¯å¢ƒæµ‹è¯•

## 10. æœ‰ç”¨çš„å‘½ä»¤

```bash
# æŸ¥çœ‹GPUä¿¡æ¯
nvidia-smi

# æŒç»­ç›‘æ§
nvidia-smi dmon -s pucvmet

# æŸ¥çœ‹CUDAç‰ˆæœ¬
nvcc --version

# æŸ¥çœ‹PyTorch CUDA
python -c "import torch; print(torch.version.cuda)"

# æ€æ‰è®­ç»ƒè¿›ç¨‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
pkill -f "python train.py"
```

å‡†å¤‡å¥½äº†å—ï¼Ÿå¼€å§‹è®­ç»ƒå§ï¼ğŸš€
