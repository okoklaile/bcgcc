# 数据集场景覆盖分析 - 快速总结

生成时间: 2026-01-24

## 📁 生成的文件

1. **COVERAGE_REPORT.md** - 详细的文字报告（英文）
2. **coverage_analysis.png** - 覆盖情况可视化总览
3. **loss_analysis.png** - 丢包场景详细分析

## 🎯 核心结论

### ✅ 您拥有的优势

```
1. 高延迟场景覆盖 - 优秀 ★★★★★
   • norway数据集: 46.93%的样本 >300ms延迟
   • NY数据集: 41.60%的样本 >300ms延迟
   • 包含极端场景（最高延迟达455秒）

2. 数据集规模 - 优秀 ★★★★★
   • 总样本数: 897,909个数据点
   • 文件数: 133个pickle文件
   • 跨越多种场景: 地铁、公交、轮渡、自行车等

3. 带宽多样性 - 良好 ★★★★☆
   • 范围: 0.09 - 3.36 Mbps (平均值)
   • 覆盖低速到中速网络
```

### ❌ 严重缺陷

```
1. 丢包场景覆盖 - 严重不足 ★☆☆☆☆
   • 只有 0.01% 的样本有非零丢包（加权平均）
   • 99%+的时间都是零丢包
   • 这是训练BC模型的最大障碍！

2. 持续拥塞场景 - 不足 ★★☆☆☆
   • 缺少长时间高丢包+高延迟的组合
   • GCC的快速回退策略难以学习
```

## 📊 各数据集特点

| 数据集 | 样本数 | 优势 | 劣势 | BC训练价值 |
|--------|--------|------|------|-----------|
| **NY** | 481,200 (53.6%) | • 最佳丢包覆盖(1.15%)<br>• 优秀高延迟覆盖<br>• 真实城市场景 | 占比过高 | ★★★★★<br>最有价值 |
| **norway** | 322,515 (35.9%) | • 最佳高延迟覆盖(46.93%)<br>• 包含断网场景 | 丢包少 | ★★★★☆<br>学习延迟处理 |
| **ghent** | 90,202 (10.0%) | • 稳定低延迟场景<br>• 多种交通工具 | 场景太理想 | ★★★☆☆<br>基础场景 |
| **opennetlab** | 3,992 (0.4%) | • 4G_3mbps有20%丢包样本<br>• 受控实验环境 | 数据量太小 | ★★★☆☆<br>特殊场景 |

## 🔥 重点文件（必须重点使用）

### 最有价值的5个文件（按丢包覆盖排序）：

```
1. opennetlab/4G_3mbps.pickle
   ✓ 20.33%的样本有丢包
   ✓ 最大丢包96%
   → 建议: 重采样50倍

2. NY/BusBrooklyn_bus57New.pickle
   ✓ 1.21%有丢包，最大99.82%
   ✓ 高延迟(平均113秒)
   → 建议: 重采样30倍

3. NY/Ferry_Ferry4.pickle
   ✓ 1.57%有丢包
   → 建议: 重采样30倍

4. NY/7Train_7trainNew.pickle
   ✓ 1.43%有丢包
   ✓ 极高延迟(最高235秒)
   → 建议: 重采样20倍

5. NY/7Train_7BtrainNew.pickle
   ✓ 极高延迟(最高455秒)
   → 建议: 重采样10倍用于延迟学习
```

## ⚠️ 对BC训练的影响

### 如果直接训练（不处理数据不平衡）：

```python
预期结果:
✓ 模型会学会在正常网络下增加带宽
✓ 模型会响应延迟上升

✗ 模型不会正确处理丢包
✗ 遇到丢包时可能继续增加带宽（雪上加霜）
✗ 无法学习AIMD策略中的"乘性减少"部分
✗ 在真实恶劣网络下表现差
```

### 风险评估：

- **中等风险**: 如果目标是"在良好网络下优化带宽利用"
- **高风险**: 如果目标是"完全替代GCC"
- **致命风险**: 如果要部署到真实生产环境

## 💡 必须采取的措施

### 1. 数据重采样（必须！）

```python
# 训练时的采样权重
sample_weights = {
    'loss_ratio == 0': 1.0,        # 正常样本
    'loss_ratio > 0': 50.0,         # 有丢包的样本
    'loss_ratio > 0.05': 100.0,     # 高丢包样本
    'delay > 500ms': 10.0,          # 高延迟样本
}

# 或者预先重复这些文件
重点文件重复次数:
- 4G_3mbps.pickle: 复制50次
- NY的Ferry和Bus文件: 复制30次
```

### 2. 损失函数加权

```python
def weighted_loss(pred, target, loss_ratio):
    base_loss = MSE(pred, target)
    
    # 对有丢包的样本加大权重
    weight = torch.where(loss_ratio > 0, 50.0, 1.0)
    
    return (base_loss * weight).mean()
```

### 3. 数据集划分策略

```python
推荐方案:
train: ghent(全部) + norway(全部) + NY(前15个) + opennetlab(6个)
val:   NY(16-20共5个文件)
test:  NY(21-25共5个文件) + opennetlab(剩余3个)

理由:
- NY有最好的丢包覆盖，用于验证和测试
- 确保测试集包含4G_3mbps.pickle
```

### 4. 数据增强（推荐）

```python
# 对现有trace添加合成丢包
def augment_with_loss(trace_data):
    # 在delay高峰时添加丢包事件
    # 模拟网络拥塞
    augmented = trace_data.copy()
    
    # 找到延迟高的时段
    high_delay_idx = np.where(trace_data['delay'] > 200)[0]
    
    # 以一定概率添加丢包
    for idx in high_delay_idx:
        if np.random.rand() < 0.3:  # 30%概率
            augmented['loss_ratio'][idx] = np.random.uniform(0.01, 0.2)
    
    return augmented
```

## 🚀 推荐的开发流程

### Phase 1: 数据准备（现在）

```bash
# 1. 提取高价值样本
python3 extract_valuable_samples.py

# 2. 创建平衡的训练集
python3 create_balanced_dataset.py \
    --oversample-loss 50 \
    --oversample-delay 10

# 3. 可视化验证
python3 plot_gcc_data.py compare \
    重点文件1 重点文件2 重点文件3 \
    重点文件对比.png
```

### Phase 2: Baseline训练（接下来）

```python
# 先用简单模型测试pipeline
1. 线性回归 baseline
2. 评估在不同场景下的表现
3. 特别关注丢包场景的MSE

目标: 建立性能基准
```

### Phase 3: BC模型训练

```python
1. MLP或LSTM模型
2. 使用加权损失函数
3. 分阶段训练：
   - Epoch 1-30: 所有数据
   - Epoch 31-60: 过采样稀有场景
   - Epoch 61-80: 只用极端场景fine-tune
```

### Phase 4: 评估和对比

```python
测试指标:
1. 整体MAE/R²
2. 分场景评估：
   - 零丢包场景
   - 低丢包场景 (0-1%)
   - 高丢包场景 (>5%)
   - 高延迟场景 (>300ms)
3. 与GCC原始决策的对比
```

## 📈 成功标准

```
必须达到:
✓ 在零丢包场景下，R² > 0.85
✓ 在有丢包场景下，预测带宽应降低（不能增加！）
✓ 高延迟时的响应速度与GCC相当

可选目标:
◯ 在某些场景下超越GCC（更平滑的带宽变化）
◯ 更好的带宽利用率
```

## 🎓 学习价值

即使数据有缺陷，这个项目仍然有价值：

1. **学习GCC在良好网络下的行为** ✓
2. **理解延迟对带宽决策的影响** ✓
3. **练习BC训练pipeline** ✓
4. **为强化学习fine-tune打基础** ✓

但不要期望：
- ❌ 直接替代GCC
- ❌ 在所有场景下都优于GCC
- ❌ 不经修改就能生产部署

## 📚 下一步阅读

1. **COVERAGE_REPORT.md** - 查看详细分析
2. **coverage_analysis.png** - 查看可视化总览
3. **loss_analysis.png** - 查看丢包场景细节

## 🤔 关键决策点

在开始训练前，您需要决定：

**决策1: 目标定位**
- [ ] A: 学习项目（接受数据限制）
- [ ] B: 生产级模型（需要更多丢包数据）

**决策2: 数据处理**
- [ ] A: 使用重采样（简单，推荐）
- [ ] B: 数据增强（更真实，但复杂）
- [ ] C: 两者结合

**决策3: 评估重点**
- [ ] A: 整体性能（可能掩盖丢包问题）
- [ ] B: 分场景评估（更全面，推荐）

---

**建议**: 先用重采样+加权损失训练一个baseline，评估在重点文件上的表现，再决定是否需要数据增强。
